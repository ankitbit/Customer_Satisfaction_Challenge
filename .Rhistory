d<-household_power_consumption$Date
d<-as.Date(household_power_consumption$Date, format = "%d/%m/%Y")
class(d)
household_power_consumption$Date<-d
household_power_consumption$Date<-as.Date(household_power_consumption$Date, format = "%d/%m/%Y")
class(household_power_consumption$Date)
data<-subset(household_power_consumption, subset = (Date >= "2007-02-01" & Date <= "2007-02-02"))
data$datetime <- strptime(paste(data$Date, data$Time), "%Y-%m-%d %H:%M:%S")
View(data)
names(household_power_consumption)
rm("data")
rm("d")
df<-subset(household_power_consumption, subset = (Date >= "2007-02-01" & Date <= "2007-02-02"))
# Convert dates and times
df$datetime <- strptime(paste(df$Date, df$Time), "%Y-%m-%d %H:%M:%S")
histo<-hist(df$Global_active_power, col = "Red", main = "Global Active Power",
xlim = range(0,6), freq = T
)
rm("household_power_consumption")
histo<-hist(df$Global_active_power, col = "Red", main = "Global Active Power",
xlim = range(0,6), freq = T, xlab = "Global Active Power (Kilowatts)"
)
household_power_consumption.txt <- read.table("household_power_consumption.txt", header = T,
sep = ";", na.strings = "?")
dir()
dev.copy(png, file = "plot1.png", height = 480, width = 480)
dev.off()
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
population_1<-rnorm(1000)
summary(population_1)
population_1<-rnorm(1000, mean = 0, sd=1)
summary(population_1)
population_1<-rnorm(1000, mean = 0.00, sd=1)
summary(population_1)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
population_2<-rnorm(1000, mean = 0.5, sd=3)
summary(population_1, population_2)
summary(population_1)
summary(population_2)
sd(population_1)
sd(population_2)
3*5
population_1<-rnorm(1000, mean = 0.00, sd=1)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
X<-vector(0, 36)
X<-vector(36)
X<-vector(36, 0)
X<-sample(population_1, 36)
summary(x)
summary(x)
x
X<-sample(population_1, 36)
X<-sample(population_1, 36)
x
x<-sample(population_1, 36)
x
summary(x)
x<-vector(length = 36)
x<-vector(0,length = 36)
x<-vector(length = 36)
x
x<-sample(population_1, 36)
rm(X)
length(x)
x<-vector(length = 36)
for(i in 1:length(x)) {
x[i]<-sample(population_1, 36)
}
sample_size<-40
number_of_samples<-50
x<-matrix(nrow = 50, ncol = 40)
sample_size<-40
number_of_samples<-50
dim(x)
for(i in 1:length(number_of_samples)) {
x[i]<-sample(population_1, sample_size)
}
summary(x[1])
summary(x[2])
summary(x[3])
for(i in 1:length(number_of_samples)) {
x[i]<-sample(population_1, sample_size, replace = T)
}
x<-matrix(nrow = 50, ncol = 40)
sample_size<-40
number_of_samples<-15
for(i in 1:length(number_of_samples)) {
x[i]<-sample(population_1, sample_size, replace = T)
}
for(i in 1:length(number_of_samples)) {
x[i]<-sample(population_1, sample_size, replace = F)
}
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(254)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(254)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(254)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
x_sample<-sample(population_1, sample_size)
y_sample<-sample(population_2, sample_size)
x_bar<-mean(x_sample)
y_bar<-mean(y_sample)
x_bar
y_bar
>var
?var
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
sample_x<-sample(population_1, sample_size)
sample_y<-sample(population_2, sample_size)
## Computing the mean of the 2 Samples of size 40
sample_mean_x<-mean(sample_x)
sample_mean_y<-mean(sample_y)
rm(histo)
rm(i)
rm(number_of_samples)
rm(x_bar)
rm(y_bar)
rm(x_sample)
rm(y_sample)
sample_variance_x<-(1/(sample_size-1))*sum((x-sample_mean_x))
2^3
2**3
sample_variance_x<-(1/(sample_size-1))*sum((x-sample_mean_x)**2)
sample_variance_y<-(1/(sample_size-1))*sum((y-sample_mean_y)**2)
sample_mean_y<-mean(sample_y)
sample_variance_y<-(1/(sample_size-1))*sum((y-sample_mean_y)**2)
sample_variance_x<-(1/(sample_size-1))*sum((sample_x-sample_mean_x)**2)
sample_variance_y<-(1/(sample_size-1))*sum((sample_y-sample_mean_y)**2)
sample_variance_x
var(population_1)
sample_variance_x<-(1/(sample_size))*sum((sample_x-sample_mean_x)**2)
sample_variance_x
sample_variance_x<-(1/(sample_size-1))*sum((sample_x-sample_mean_x)**2)
sample_variance_y<-(1/(sample_size-1))*sum((sample_y-sample_mean_y)**2)
T_n<-sample_variance_x+(1/3)sample_variance_y
T_n<-sample_variance_x+(1/3)*sample_variance_y
T_n
sigma<-sqrt(var(population_1))
sigma
sd(population_1)
V_n<-(1/(2*sigma))*sqrt(sample_size)*(sample_mean_x - sample_mean_y - mean(population_1) +mean(population_2))
mean(V_n)
sd(V_n)
sqrt(var(V_n))
V_n
set.seed(124)
population_1<-rnorm(1000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(1000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
number_of_samples<-44
x<-rep(0,50)
number_of_samples<-50
sample_x<-matrix(rep(0, 200), nrow = 50, ncol = 40)
dim(sample_x)
dim(sample_x)[1]
set.seed(124)
population_1<-rnorm(100000, mean = 0.00, sd=1)
set.seed(154)
population_2<-rnorm(100000, mean = 0.5, sd=3*sd(population_1))
sd(population_1)
sd(population_2)
sample_size<-40
number_of_samples<-50
for(i in 1:dim(sample_x)[1]) {
sample_x[i]<-sample(population_1, 40)
}
warnings()
View(sample_x)
for(i in 1:dim(sample_x)[1]) {
sample_x[i,]<-sample(population_1, 40)
}
View(sample_x)
sample_y<-matrix(rep(0, 200), nrow = 50, ncol = 40)
for(i in 1:dim(sample_y)[1]) {
sample_y[i,]<-sample(population_2, 40)
}
View(sample_y)
sample_means_x<-rep(0,50)
sample_means_y<-rep(0,50)
length(sample_means_x)
z<-1:40
lapply(z[1:3], mean)
z<-matrix(1:24, nrow = 4)
z
apply(z,1, mean)
apply(z,2, mean)
apply(z,1, mean)
(15+13+17+21)/6
z<-apply(sample_x,1,mean )
head(z)
sample_means_y<-apply(sample_y,1,mean )
head(sample_means_y)
rm(sample_mean_x)
rm(sample_mean_y)
mean(sample_means_x)
mean(sample_means_y)
sample_variances_x<-rep(0,50)
sample_variances_x<-rep(0,50)
sample_means_x
l<-2
m<-3
diff(l,m)
rm(l,m)
head(sample_x[1,])
sample_x[1,]
mean(sample_x[1,])
mean(sample_x[1,])==sample_means_x[1]
sample_means_x[1]
sample_means_x<-apply(sample_x,1,mean )
sample_means_x[1]
mean(sample_x[1,])==sample_means_x[1]
l<-1:4
m<-0:3
l-m
dim(sample_x[1])
length(sample_x[1])
length(sample_x[1,])
sample_x_mean_deviation<- matrix(rep(0,200), nrow = 50, ncol = 40)
mean(sample_x[1])
mean(sample_x[1,])
for(i in 1:dim(sample_x)[1]) {
m<-mean(sample_x[i,])
sample_x_mean_deviation[i,]<-sample_x[i,] - m
}
sample_variances_x<-rep(0,50)
class(sample_x_mean_deviation[1,])
l
l<-matrix(1:24, nrow = 4)
l<-1:4
(l-1)**2
for(i in 1:dim(sample_x)[1]) {
m<-mean(sample_x[i,])
sample_x_mean_deviation[i,]<-(sample_x[i,] - m)**2
}
for(i in 1:dim(sample_x)[1]) {
m<-mean(sample_x[i,])
sample_x_mean_deviation[i,]<-(1/(sample_size-1))*(sample_x[i,] - m)**2
}
sample_variances_x<-apply(sample_x_mean_deviation, 1, sum)
sample_variances_x
##Matrix of squared mean deviations divided by (1/n-1) for Y's
sample_y_mean_deviation<- matrix(rep(0,200), nrow = 50, ncol = 40)
for(i in 1:dim(sample_y)[1]) {
m<-mean(sample_y[i,])
sample_y_mean_deviation[i,]<-(1/(sample_size-1))*(sample_y[i,] - m)**2
}
sample_variances_y<-rep(0,50)
sample_variances_y<-apply(sample_y_mean_deviation, 1, sum)
T_n<- sample_variances_x + (1/3)*sample_variances_y
sigma<-sqrt(var(population_1))
V_n<- (1/(2*sigma))*sqrt(number_of_samples)*(sample_means_x - sample_means_y - mean(population_1) +mean(population_2))
mean(V_n)
var(V_n)
?sd
sd(V_n)
hist(V_n)
density(V_n)
plot(density(V_n))
library(tm)
library(qdap)
library(SnowballC)
library(purrr)
library(text2vec)
install.packages("text2vec")
library(text2vec)
library(e1071) # for naive bayes
library(xgboost)
getwd()
setwd("~/Customer_Satisfaction_Challenge")
getwd()
train <- fread("train.csv")
library(data.table)
train <- fread("train.csv")
test <- fread("test.csv")
cleanData <- function(data)
{
data[, Description := map_chr(Description, tolower)] # to lower
data[, Description := map_chr(Description, function(k) gsub(pattern = "[[:punct:]]",replacement = "",x = k))] # remove punctuation
data[, Description := map_chr(Description, function(k) gsub(pattern = "\\d+",replacement = "",x = k))] # remove digits
data[, Description := map_chr(Description, function(k) replace_abbreviation(k))] # Sr. to Senior
data[, Description := map_chr(Description, function(k) replace_contraction(k))] # isn't to is not
data[,Description := map(Description, function(k) rm_stopwords(k, Top200Words, unlist = T))] # remove stopwords
data[, Description := map(Description, function(k) stemmer(k))] # played, plays to play
data[, Description :=  map(Description, function(k) k[nchar(k) > 2])] # remove two alphabet words like to, ok, po
return (data)
}
train_clean <- cleanData(train)
test_clean <- cleanData(test)
## Clean Data -----------------------------------------------------------
cleanData <- function(data)
{
data[, Description := map_chr(Description, tolower)] # to lower
data[, Description := map_chr(Description, function(k) gsub(pattern = "[[:punct:]]",replacement = "",x = k))] # remove punctuation
data[, Description := map_chr(Description, function(k) gsub(pattern = "\\d+",replacement = "",x = k))] # remove digits
data[, Description := map_chr(Description, function(k) replace_abbreviation(k))] # Sr. to Senior
data[, Description := map_chr(Description, function(k) replace_contraction(k))] # isn't to is not
data[,Description := map(Description, function(k) rm_stopwords(k, Top200Words, unlist = T))] # remove stopwords
data[, Description := map(Description, function(k) stemmer(k))] # played, plays to play
data[, Description :=  map(Description, function(k) k[nchar(k) > 2])] # remove two alphabet words like to, ok, po
return (data)
}
train_clean <- cleanData(train)
test_clean <- cleanData(test)
# Bag of Words ------------------------------------------------------------
## Bag of words technique converts the list of tokens (words) into a separate column with binary values in it.
## Lets understand it.
ctext <- Corpus(VectorSource(train_clean$Description))
tdm = DocumentTermMatrix(ctext)
print(tdm)
# let's see how BOW looks like - every column becomes one feature
inspect(tdm[1:10,1:5])
## From here, we'll use text2vec package which provides immense potential for feature engineering
## we'll build two models
# a) On Bag of Words Corpus
# b) On TF-IDF Corpus
# c) 2 Gram Model - Your to-do  Task
# You can read more about TF-IDF here: http://www.tfidf.com/
## Bag of Words Model
trte_data <- rbind(train[,.(User_ID, Description)], test[,.(User_ID, Description)])
trte_data$Description <- unlist(map(trte_data$Description, paste, collapse = ","))
bow <- itoken(trte_data$Description, preprocessor = tolower ,tokenizer = word_tokenizer, ids = trte_data$User_ID)
bow_vocab <- create_vocabulary(bow)
bow_vocab # now we have converted the text into tokens. woah! every word can be converted into a feature
## But not all words will be important, Are they ? let's remove words which occur less than 200 times in whole data
pruned_bow <- prune_vocabulary(bow_vocab, term_count_min = 100)
pruned_bow
# get these vocabulary in a data frame for model training
vovec <- vocab_vectorizer(pruned_bow)
dtm_text <- create_dtm(bow, vovec)
feats <- as.data.table(as.matrix(dtm_text))
feats[1:10,1:5] # see 1st 10 rows and 1st 5 columns
# first feature set
train_feats <- feats[1:nrow(train)]
test_feats <- feats[(nrow(train)+1):nrow(feats)]
cols <- setdiff(colnames(train), c('User_ID','Is_Response','Description'))
for(x in cols)
{
if (class(train[[x]]) == 'character')
{
levels <- unique(c(train[[x]], test[[x]]))
train[[x]] <- as.numeric(factor(train[[x]], levels = levels))
test[[x]] <- as.numeric(factor(test[[x]], levels = levels))
}
}
## preparing data for training
train_feats <- cbind(train_feats, train[,.(Browser_Used, Device_Used, Is_Response)])
test_feats <- cbind(test_feats, test[,.(Browser_Used, Device_Used)])
train_feats[, Is_Response := ifelse(Is_Response == 'happy',1,0)]
train_feats[, Is_Response := as.factor(Is_Response)]
## naive Bayes is known to perform quite well in text classification problems
model <- naiveBayes(Is_Response ~ ., data = train_feats, laplace = 1)
preds <- predict(model, test_feats)
# make your submission
sub <- data.table(User_ID = test$User_ID, Is_Response = ifelse(preds == 1, "happy", "not_happy"))
fwrite(sub, "sub1.csv")
# TF -TDF Model -----------------------------------------------------------
TIDF <- TfIdf$new()
dtm_text_tfidf <- fit_transform(dtm_text, TIDF)
feats <- as.data.table(as.matrix(dtm_text_tfidf))
# second feature set
train_feats <- feats[1:nrow(train)]
test_feats <- feats[(nrow(train)+1):nrow(feats)]
## preparing data for training
train_feats <- cbind(train_feats, train[,.(Browser_Used, Device_Used, Is_Response)])
test_feats <- cbind(test_feats, test[,.(Browser_Used, Device_Used)])
train_feats[, Is_Response := ifelse(Is_Response == "happy",1,0)]
## You can use naiveBayes Model here and compare the accuracy.
## let's try xgboost model here.
# set parameters for xgboost
param <- list(booster = "gbtree",
objective = "binary:logistic",
eval_metric = "error",
#num_class = 9,
eta = .2,
# gamma = 1,
max_depth = 6,
min_child_weight = 0,
subsample = .8,
colsample_bytree = .3
)
## function to return predictions using best CV score
predictions <- c()
give_predictions <- function(train, test, params, iters)
{
dtrain <- xgb.DMatrix(data = as.matrix(train[,-c('Is_Response'),with=F]), label = train_feats$Is_Response)
dtest <- xgb.DMatrix(data = as.matrix(test))
cv.model <- xgb.cv(params = params
,data = dtrain
,nrounds = iters
,nfold = 5L
,stratified = T
,early_stopping_rounds = 40
,print_every_n = 20
,maximize = F)
best_it <- cv.model$best_iteration
best_score <- cv.model$evaluation_log$test_error_mean[which.min(cv.model$evaluation_log$test_error_mean)]
cat('CV model returned',best_score,'error score')
tr.model <- xgb.train(params = param
,data = dtrain
,nrounds = best_it
,watchlist = list(train = dtrain)
,print_every_n = 20
)
preds <- predict(tr.model, dtest)
predictions <- append(predictions, preds)
return(predictions)
}
# get predictions
my_preds <- give_predictions(train_feats, test_feats, param, 1000)
## create submission file
preds <- ifelse(my_preds > 0.66,1,0) #cutoff threshold
sub2 <- data.table(User_ID = test$User_ID, Is_Response = preds)
fwrite(sub2, "sub2.csv")
## What's Next ?
## Till now, we made 1-gram model i.e. one word per column. We can extend it to 2-3-n gram
## create another model with 2-gram features
gr_vocab <- create_vocabulary(bow, ngram = c(1L,2L))
